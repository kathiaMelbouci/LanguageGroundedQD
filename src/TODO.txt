
1̶.̶ ̶A̶d̶d̶ ̶a̶ ̶s̶c̶r̶i̶p̶t̶ ̶t̶h̶a̶t̶ ̶l̶o̶a̶d̶s̶ ̶a̶n̶ ̶a̶r̶c̶h̶i̶v̶e̶,̶ ̶p̶l̶a̶y̶s̶ ̶i̶t̶,̶ ̶a̶n̶d̶ ̶v̶e̶r̶i̶f̶i̶e̶s̶ ̶t̶h̶a̶t̶ ̶t̶h̶e̶ ̶f̶i̶n̶a̶l̶ ̶B̶D̶ ̶i̶s̶ ̶t̶h̶e̶ ̶s̶a̶m̶e̶ ̶t̶h̶a̶t̶ ̶i̶s̶ ̶i̶n̶ ̶t̶h̶e̶ ̶a̶r̶c̶h̶i̶v̶e̶ ̶f̶o̶r̶ ̶e̶a̶c̶h̶ ̶p̶o̶l̶i̶c̶y̶
2̶.̶ ̶A̶d̶d̶ ̶a̶ ̶s̶c̶r̶i̶p̶t̶ ̶t̶h̶a̶t̶ ̶s̶a̶v̶e̶s̶ ̶e̶n̶t̶i̶r̶e̶ ̶t̶r̶a̶j̶e̶c̶t̶o̶i̶r̶s̶ ̶(̶a̶c̶t̶i̶o̶n̶,̶ ̶o̶b̶s̶)̶ ̶t̶o̶ ̶t̶h̶e̶ ̶a̶r̶c̶h̶i̶v̶e̶ 
3.̶ ̶A̶d̶d̶ ̶a̶ ̶s̶c̶r̶i̶p̶t̶ ̶t̶h̶a̶t̶ ̶l̶o̶a̶d̶ ̶t̶h̶a̶t̶ ̶a̶r̶c̶h̶i̶v̶e̶ ̶a̶n̶d̶ ̶t̶h̶e̶n̶ ̶a̶n̶n̶o̶t̶a̶t̶e̶s̶ ̶i̶t̶ ̶w̶i̶t̶h̶ ̶s̶e̶m̶a̶n̶t̶i̶c̶ ̶i̶n̶f̶o̶ ̶(̶y̶o̶u̶ ̶m̶i̶g̶h̶t̶ ̶n̶e̶e̶d̶ ̶t̶o̶ ̶d̶o̶ ̶a̶ ̶b̶i̶t̶ ̶o̶f̶ ̶m̶a̶n̶u̶a̶l̶ ̶d̶e̶f̶i̶n̶i̶n̶g̶ ̶h̶e̶r̶e̶)
   3̶.̶1̶ ̶I̶'̶v̶e̶ ̶a̶d̶d̶e̶d̶ ̶t̶h̶e̶ ̶n̶e̶w̶ ̶e̶n̶v̶i̶r̶o̶n̶m̶e̶n̶t̶ ̶w̶i̶t̶h̶ ̶b̶o̶u̶n̶d̶i̶n̶g̶ ̶b̶o̶x̶e̶s̶ ̶a̶n̶d̶ ̶v̶e̶c̶t̶o̶r̶ ̶i̶m̶a̶g̶e̶s̶ ̶(̶u̶n̶d̶e̶r̶ ̶c̶r̶e̶a̶t̶i̶v̶e̶ ̶c̶o̶m̶m̶o̶n̶s̶)̶ ̶+̶ ̶a̶d̶d̶e̶d̶ ̶t̶h̶e̶ ̶c̶o̶l̶o̶r̶ ̶t̶i̶l̶i̶n̶g̶
       -̶ ̶I̶ ̶n̶e̶e̶d̶ ̶t̶o̶ ̶a̶d̶d̶ ̶a̶n̶ ̶i̶m̶a̶g̶e̶ ̶t̶i̶l̶i̶n̶g̶ ̶t̶o̶ ̶t̶h̶e̶ ̶S̶c̶e̶n̶e̶ ̶a̶n̶d̶ ̶c̶h̶e̶c̶k̶ ̶o̶n̶ ̶w̶h̶i̶c̶h̶ ̶t̶i̶l̶e̶ ̶t̶h̶e̶ ̶p̶o̶i̶n̶t̶s̶ ̶f̶a̶l̶l̶
       -̶ ̶N̶a̶v̶i̶g̶a̶t̶i̶o̶n̶E̶n̶v̶ ̶n̶e̶e̶d̶s̶ ̶t̶o̶ ̶h̶a̶v̶e̶ ̶o̶n̶e̶ ̶o̶f̶ ̶t̶h̶o̶s̶e̶ ̶S̶c̶e̶n̶c̶e̶ ̶o̶b̶j̶e̶c̶t̶s̶,̶ ̶a̶n̶d̶ ̶u̶s̶e̶ ̶i̶t̶ ̶t̶o̶ ̶a̶n̶n̶o̶t̶a̶t̶e̶ ̶t̶r̶a̶j̶e̶c̶t̶o̶r̶i̶e̶s̶.̶
       -̶ ̶I̶ ̶t̶h̶i̶n̶k̶ ̶t̶h̶a̶t̶ ̶t̶h̶e̶ ̶a̶n̶n̶o̶t̶a̶t̶i̶o̶n̶ ̶h̶e̶r̶e̶ ̶n̶e̶e̶d̶s̶ ̶t̶o̶ ̶b̶e̶ ̶c̶o̶a̶r̶s̶e̶r̶,̶ ̶i̶.̶e̶.̶ ̶i̶t̶ ̶s̶h̶o̶u̶l̶d̶n̶'̶t̶ ̶b̶e̶ ̶m̶a̶d̶e̶ ̶o̶n̶ ̶a̶l̶l̶ ̶t̶i̶m̶e̶s̶t̶e̶p̶s̶.̶ ̶M̶a̶y̶b̶e̶ ̶j̶u̶s̶t̶ ̶k̶e̶e̶p̶ ̶t̶h̶e̶ ̶t̶i̶m̶e̶s̶t̶e̶p̶s̶ ̶a̶t̶ ̶w̶h̶i̶c̶h̶ ̶w̶e̶ ̶r̶e̶a̶c̶h̶ ̶a̶ ̶p̶o̶i̶n̶t̶ ̶w̶i̶t̶h̶ ̶n̶e̶w̶ ̶s̶e̶m̶a̶n̶t̶i̶c̶ ̶i̶n̶f̶o̶,̶ ̶f̶o̶r̶
         e̶x̶a̶m̶p̶l̶e̶:̶ ̶(̶t̶i̶m̶e̶s̶t̶e̶p̶_̶1̶5̶:̶ ̶"̶n̶o̶r̶t̶h̶ ̶o̶f̶ ̶f̶r̶i̶d̶g̶e̶"̶,̶ ̶t̶i̶m̶e̶s̶t̶e̶p̶_̶2̶1̶:̶ ̶"̶"̶,̶ ̶t̶i̶m̶e̶s̶t̶e̶p̶_̶3̶2̶:̶ ̶"̶s̶o̶u̶t̶h̶_̶w̶e̶s̶t̶_̶o̶f̶_̶b̶a̶t̶h̶t̶u̶b̶"̶,̶ ̶t̶i̶m̶e̶s̶t̶e̶p̶_̶3̶3̶:̶ ̶"̶w̶e̶s̶t̶ ̶o̶f̶ ̶b̶a̶t̶h̶t̶u̶b̶"̶,̶ ̶.̶.̶.̶.̶)̶
       -̶ ̶I̶ ̶t̶h̶i̶n̶k̶ ̶y̶o̶u̶ ̶c̶a̶n̶ ̶m̶a̶p̶ ̶t̶r̶a̶j̶e̶c̶t̶o̶r̶i̶e̶s̶ ̶t̶o̶ ̶m̶a̶n̶u̶a̶l̶ ̶d̶e̶s̶c̶r̶i̶p̶t̶i̶o̶n̶s̶:̶ ̶e̶.̶g̶.̶ ̶y̶o̶u̶ ̶c̶a̶n̶ ̶s̶a̶y̶ ̶t̶h̶e̶ ̶t̶r̶a̶j̶ ̶g̶o̶e̶s̶ ̶u̶p̶ ̶f̶r̶o̶m̶ ̶t̶h̶e̶ ̶i̶n̶i̶t̶i̶a̶l̶ ̶p̶o̶i̶n̶t̶ ̶x̶ ̶i̶f̶ ̶i̶t̶ ̶n̶e̶v̶e̶r̶ ̶g̶o̶e̶s̶ ̶d̶o̶w̶n̶ ̶t̶h̶e̶r̶e̶,̶ ̶b̶u̶t̶ ̶y̶e̶a̶h̶,̶ ̶t̶h̶a̶t̶ ̶m̶i̶g̶h̶t̶ ̶b̶e̶ ̶a̶m̶b̶i̶g̶u̶o̶u̶s̶.̶.̶.̶
4̶.̶ ̶A̶d̶d̶ ̶t̶h̶e̶ ̶l̶l̶l̶m̶ ̶p̶r̶o̶m̶p̶t̶i̶n̶g̶ ̶p̶a̶r̶t̶ ̶t̶o̶ ̶g̶e̶n̶e̶r̶a̶t̶e̶ ̶d̶e̶s̶c̶r̶i̶p̶t̶i̶o̶n̶s̶ ̶f̶o̶r̶ ̶e̶a̶c̶h̶ ̶o̶f̶ ̶t̶h̶e̶ ̶t̶r̶a̶j̶e̶c̶t̶o̶r̶i̶e̶s̶
5̶.̶ ̶V̶e̶r̶i̶f̶y̶ ̶d̶a̶t̶a̶s̶e̶t̶

-̶ ̶W̶h̶e̶n̶e̶v̶e̶r̶ ̶y̶o̶u̶ ̶g̶e̶t̶ ̶t̶h̶e̶ ̶o̶p̶p̶o̶r̶t̶u̶n̶i̶t̶y̶ ̶t̶o̶ ̶d̶e̶s̶c̶r̶i̶b̶e̶ ̶t̶h̶e̶ ̶c̶o̶m̶p̶l̶e̶t̶e̶ ̶a̶r̶c̶h̶i̶v̶e̶ ̶=̶>̶ ̶u̶p̶l̶o̶a̶d̶ ̶i̶t̶ ̶s̶o̶m̶e̶w̶h̶e̶r̶e̶ ̶(̶c̶r̶e̶a̶t̶e̶ ̶a̶ ̶g̶i̶t̶ ̶r̶e̶l̶e̶a̶s̶e̶?̶ ̶O̶r̶ ̶m̶a̶y̶b̶e̶ ̶p̶u̶t̶ ̶i̶t̶ ̶d̶i̶r̶e̶c̶t̶l̶y̶ ̶o̶n̶ ̶h̶u̶g̶g̶i̶n̶g̶f̶a̶c̶e̶?̶)̶

-̶ ̶A̶d̶d̶ ̶t̶h̶e̶ ̶c̶o̶d̶e̶ ̶s̶t̶r̶u̶c̶t̶u̶r̶e̶ ̶f̶o̶r̶ ̶g̶e̶t̶t̶i̶n̶g̶ ̶t̶e̶x̶t̶ ̶e̶m̶b̶e̶d̶d̶i̶n̶g̶s̶,̶ ̶m̶a̶k̶e̶ ̶i̶t̶ ̶m̶o̶d̶e̶l̶ ̶a̶g̶n̶o̶s̶t̶i̶c̶ ̶a̶n̶d̶ ̶e̶a̶s̶i̶l̶y̶ ̶c̶o̶n̶f̶i̶g̶u̶r̶a̶b̶l̶e̶
-̶ ̶R̶e̶a̶d̶ ̶t̶h̶e̶ ̶c̶o̶d̶e̶ ̶o̶f̶ ̶d̶e̶c̶i̶s̶i̶o̶n̶ ̶t̶r̶a̶n̶s̶f̶o̶r̶m̶er => that was not super necesssary, it's essentially miniGPT 
-̶ ̶R̶e̶a̶d̶ ̶t̶h̶e̶ ̶c̶o̶d̶e̶ ̶o̶f̶ ̶n̶a̶n̶o̶G̶P̶T̶
  A'ight! I think I have a decent understanding of it (I say decent because some parallelism stuff like ddp and things like gradient clipping or TF32 datatypes are a bit new to me)
  So, now, I think I can just instantiate a nanoGPT which because of the low vocabulary size of my toy dataset, will probably run only on a single GPU.
  [TODO]: The main questions are 

          1. I know how to use torch.nn.Embedding to map sequence ids to learnable embeddings. Now, I also need to generate embeddings for actions, sequences and behaviors.
             How do I do that? 

             I have a sequence 

                 [text_token_1 ... text_token_n] + [o_0, a_0, ..., o_n, a_n]  #or I can set [bd, o_0, a_0, bd, o_1, a_1, ..., bd, o_n, a_n] for the second list

             The text_token_i will be mapped by a torch.nn.Embedding to [batch_size, sequence_len, embed_sz]
             I think that I can do exactly the same thing for o_i, a_i, bd.
             Then I can concatenate all of those. When going over the max token size, I'll just drop previous o_i, a_i, bd.

             The block_size is just about the maximum number of tokens, and since episodes are 400 steps long, the full sequence will be dim(a)*400+dim(o)*400+dim(bd)*400
             which in the toy environment will be 2*400+5*400+2*400=3600. As the archive has llm descriptions that can reach 206 tokens with the learned tokenizer,
             the total maximum possible sequence lenght is 3600+206=3806. This is higher than the default block_size of 1024 in nanoGPT, but a short sliding window 
             from _tau would suffice. The environment being simple, I'd say that even a sequence length of ~15 should largely suffice. So, in practice, it's likely
             that we'd get good results with a block size of around 350.

             Well, that's answered!

          2. how to superwise? Contrary to the original nanoGPT, the targets are not going to be (one-hot) distributions over the vocabulary. Instead, they are going to be 
             actions. I could start with a simple MSE as the env is super simple, and then see if I can use more sophisticated methods later (like the paper that clusterd actions 
             and then added offsets - the reference is in my bibliography repo, it's called something like k-shot transformers or whatnot)

          3. For training, I think that as the problem is very simple for now, I will not use any ddp stuff and will train on a single machine. Also, I don't think
             that things like microbatches and gradient accumulation will be necessary. The same can be said about gradient scaling (e.g. torch.cuda.amp.GradScaler)
             as I don't need to use float16, and I don't have bfloat16 or bfloat32 on the gpus I have access to for now. I'll just go with float32.

             so yeah: todo: rewrite the train loop

          4. Once the model is trained, how should I evaluate it?

             Two different eval metrics: when conditioned on a behavior + text, does the policy i) reach the behavior ii) do what the language instructions ask it to do?
             For this second one, two different strategies: ii.A (dumb) annotate the trajectory, see if the annotations are similar to the ones already associated to the archive.
                                                               Just see if the objects/colors appear, and if they appear in the right order.
                                                            ii.B generate an annotation, and ask an LLM if this annotation and the textual prompt match
                                                            iii.C Some qualitative evaluations


-̶ ̶R̶e̶a̶d̶ ̶t̶h̶e̶ ̶p̶a̶p̶e̶r̶ ̶t̶h̶a̶t̶ ̶c̶o̶n̶d̶i̶t̶i̶o̶n̶s̶ ̶d̶e̶c̶i̶s̶i̶o̶n̶ ̶t̶r̶a̶n̶s̶f̶o̶r̶m̶e̶r̶ ̶o̶n̶ ̶l̶a̶n̶g̶u̶a̶g̶e̶ ̶(̶P̶r̶e̶t̶r̶a̶i̶n̶i̶n̶g̶ ̶f̶o̶r̶ ̶L̶a̶n̶g̶u̶a̶g̶e̶-̶C̶o̶n̶d̶i̶t̶i̶o̶n̶e̶d̶ ̶I̶m̶i̶t̶a̶t̶i̶o̶n̶ ̶w̶i̶t̶h̶ ̶T̶r̶a̶n̶s̶f̶o̶r̶m̶e̶r̶s̶)̶

-̶ ̶l̶e̶a̶r̶n̶ ̶a̶ ̶t̶o̶k̶e̶n̶i̶z̶e̶r̶:̶    

    The set of relevant words used to describe/instruct in the context of the toy environment is 
    very restricted, so while one could use a more powerful pretrained model, it seems wasetful
    not to see what one can obtain by just learnng a specialized vocabulary and thus tokenizer.
    Many words such as "jump", "dip", "route" that appear in the desciptions generated by the LLM
    are not useful at all for shaping the trajectory in that environment. 

    That being said, it would be interesting to compare with results when something like BERT is used.

    with that vocabulary size (~900), the number of parameters of the GPT model used by decision transformers is about 2.1 millions
    he maximum sequence length in the archive with the learned tokenizer is 206. The default block_size of nanoGPT is 1024. This leaves plenty of room for adding as sliding window from the 
            _tau sequence.

-̶ ̶d̶e̶c̶i̶d̶e̶ ̶o̶n̶ ̶a̶n̶ ̶i̶n̶i̶t̶i̶a̶l̶ ̶n̶e̶t̶w̶o̶r̶k̶ ̶a̶r̶c̶h̶i̶t̶e̶c̶t̶u̶r̶e̶,̶ ̶t̶r̶y̶ ̶t̶o̶ ̶d̶e̶d̶u̶c̶e̶ ̶n̶e̶t̶w̶o̶r̶k̶ ̶s̶i̶z̶e̶ ̶b̶y̶ ̶c̶o̶m̶p̶a̶r̶i̶n̶g̶ ̶t̶h̶e̶ ̶p̶r̶o̶b̶l̶e̶m̶s̶ ̶u̶s̶e̶d̶ ̶i̶n̶ ̶d̶e̶c̶i̶s̶i̶o̶n̶ ̶t̶r̶a̶n̶s̶f̶o̶r̶m̶e̶r̶ ̶w̶i̶t̶h̶ ̶y̶o̶u̶r̶s̶.̶ ̶C̶o̶m̶p̶a̶r̶i̶s̶o̶n̶s̶ ̶s̶h̶o̶u̶l̶d̶ ̶b̶e̶ ̶m̶a̶d̶e̶ ̶i̶n̶ ̶t̶e̶r̶m̶s̶ ̶o̶f 
  ̶-̶ ̶e̶p̶i̶s̶o̶d̶e̶ ̶l̶e̶n̶g̶t̶h̶
 ̶ ̶-̶ ̶a̶c̶t̶i̶o̶n̶/̶o̶b̶s̶ ̶d̶i̶m̶e̶n̶s̶i̶o̶n̶s̶
 ̶ ̶-̶ ̶d̶i̶v̶e̶r̶s̶i̶t̶y̶ ̶o̶f̶ ̶p̶o̶s̶s̶i̶b̶l̶e̶ ̶b̶e̶h̶a̶v̶i̶o̶r̶s̶
 ̶ ̶-̶ ̶T̶h̶e̶ ̶o̶v̶e̶r̶a̶l̶l̶ ̶c̶o̶m̶p̶l̶e̶x̶i̶t̶y̶ ̶o̶f̶ ̶t̶h̶e̶ ̶d̶y̶n̶a̶m̶i̶c̶s̶
 ̶ ̶-̶ ̶T̶h̶e̶ ̶s̶i̶z̶e̶ ̶o̶f̶ ̶t̶h̶e̶ ̶n̶a̶t̶u̶r̶a̶l̶ ̶l̶a̶n̶g̶u̶a̶g̶e̶ ̶e̶m̶b̶e̶d̶d̶i̶n̶g̶s̶

    The environment is considerably less complex than frostbite, which is used by TDT. They use n_layer=6, n_head=8, n_embd=128, with a large vocabulary size. The vocabulary size I have
    is very small, and that already reduces network size considerably. I could try to just set n_layer=4, n_head=6, n_embd=128 an see where it goes.

- Make sure training works on a small dataset (no obvious bugs)
-̶ ̶L̶a̶u̶n̶c̶h̶ ̶t̶r̶a̶i̶n̶i̶n̶g̶ ̶o̶n̶ ̶s̶e̶r̶v̶e̶r̶!

  TODO: check that loading function does indeed load additional heads etc that you've added
        also, weight tying might not make sense here


- If training goes well, start writing tiny paper
