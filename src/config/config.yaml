---
train_model: True #if False, a pretrained model should be provided.
test_model: False #
deploy_in_env: True 
model_cfg: 
  learned_tokenizer: "<path_to_tokenizer>"
  kmeans:  "<path_to_kmeans_file>" 
  block_size: 1435 
  n_layer: 2
  n_head: 4
  n_embd:  360
  bias: 0 
  dropout_p: 0.1 
  replace_head:
    cluster_idx_heads: 0 # if >0, replaces the corresponding heads with an RLMLP with h_dim specified by this value
    cluster_offset_head: 0 # if >0, replaces the corresponding heads with an RLMLP with h_dim specified by this value
input_normalization:
  normalize: {"bds": True, "obs": True} #action normalization is unavailable
  env_type: "navigation_env" #only needed if the normalize dict above has keys that are True. Currenlty, the only available choice in 'navigation_env'
train_cfg:
  data_path_train: "<path_to_train_data>"
  data_path_val: "<path_to_val_data>"
  max_epochs: 5000 
  val_frequ: 5 
  batch_size: 30
  ablate_prompt: False 
  adamW: 
      learning_rate : 5e-3 # max learning rate 
      weight_decay : 1e-1
      beta1 : 0.9
      beta2 : 0.95
      grad_clip : 0.0 # clip gradients at this value, or disable if == 0.0
  schedule:
    decay_lr: True
    lr_decay_steps : 300000 # 
    warmup_steps: 1000 # how many steps to warm up for
    min_lr : 5e-5 
  only_train: {"cluster_idx_heads": False, "cluster_offset_head": False} #disables backbone training and only opitmizes the chosen heads. Not that both values can be True at once.
test_cfg:
  data_path: "<path_to_test_data>"
  #you can provide a list of indexes into the test archive that will be skipped
  exclude_file: "/tmp/exclude_file.json" 
  batch_size: 16
  ablate_prompt: False
deploy_cfg:
  env_type: "navigation_env"
  #the file should contain a List[List] prompt_lst, such that prompt_lst[i]=[textual_conditioning, bd_0, ..., bd_n] for n-dimensional bds
  #use --export_as_prompt_list from data_tools.py
  prompts: "<path_to_prompt_list>" 
  generation_strategy: "sample" 
  ablate_text: False
  ablate_bd: False 
device: "cuda"
dtype: "float32" 
pretrained_model: ""
seed: 0
logging: 
  log_dir: "<path_to_log_dir>
