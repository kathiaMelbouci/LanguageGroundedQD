---
train_model: True #if False, a pretrained model should be provided.
test_model: True  #
model_cfg: 
  learned_tokenizer: "/home/achkan/misc_libs/LanguageGroundedQD_venvs/venv_1/src/LanguageGroundedQD/src/models/tokenizers/tokenizer_XT7jMcHDYG/" 
  block_size: 402
  n_layer: 4
  n_head: 4
  n_embd: 128 
  bias: 0 #the original nanoGPT code says that turning it off produces better results
  dropout_p: 0.0 #nanoGPT original code says 0.0 is good for pre-training, 0.1 is good for fine-tuning
input_normalization:
  #Not sure if necessary. Even though input embeddings result from MLPs, attention blocks start with layernorm.
  normalize: {"bds": True, "obs": True, "act": False}
  env_type: "navigation_env" #only needed if the normalize dict above has keys that are True. Currenlty, the only available choice in 'navigation_env'
train_cfg:
  data_path_train: "/home/achkan/misc_libs/ToyEnvironmentDescribedPolicyRepertoire/described_archive_full_train.pkl"
  data_path_val: "/home/achkan/misc_libs/ToyEnvironmentDescribedPolicyRepertoire/described_archive_full_val.pkl"
  max_epochs: 10 # total number of training iterations
  val_frequ: 3 #compute validation loss only every val_frequ iterations
  batch_size: 4
  adamW: 
      learning_rate : 6e-4 # max learning rate
      weight_decay : 1e-1
      beta1 : 0.9
      beta2 : 0.95
      grad_clip : 0.0 # clip gradients at this value, or disable if == 0.0
  schedule:
    decay_lr : True # whether to decay the learning rate
    lr_decay_steps : 600000 # should be ~= max_iters per Chinchilla
    warmup_steps: 2000 # how many steps to warm up for
    min_lr : 6e-5 # minimum learning rate, should be ~= learning_rate/10 per Chinchilla
test_cfg:
  data_path: "/home/achkan/misc_libs/ToyEnvironmentDescribedPolicyRepertoire/described_archive_full_test.pkl"
  batch_size: 4
device: "cuda"
dtype: "float32" 
pretrained_model: ""
deterministic: True
logging: 
  log_dir: "/Desktop/tmp_desktop/"

