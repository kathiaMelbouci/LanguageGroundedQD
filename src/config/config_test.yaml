---
train_model: False #if False, a pretrained model should be provided.
test_model: False #
deploy_in_env: True 
model_cfg: 
  learned_tokenizer: "/home/achkan/misc_libs/LanguageGroundedQD_venvs/venv_1/src/LanguageGroundedQD/src/models/tokenizers/tokenizer_hz7RY06aNX"
  kmeans:  "/home/achkan/misc_libs/LanguageGroundedQD_venvs/venv_1/src/LanguageGroundedQD/src/models/kmeans/cluster_centers_per_dim_4_4.pkl"
  #kmeans:  "/home/achkan/misc_libs/LanguageGroundedQD_venvs/venv_1/src/LanguageGroundedQD/src/models/kmeans/cluster_centers_per_dim_6_5.pkl"
  #kmeans:  "/home/achkan/misc_libs/LanguageGroundedQD_venvs/venv_1/src/LanguageGroundedQD/src/models/kmeans/cluster_centers_per_dim_10_8.pkl"
  block_size: 1435 #In the current dataset, the max token length is 226, and the toy env's episode length is 400 (=>1200 tokens), so block_size needs only be larger than 1426.
  n_layer: 2
  n_head: 4
  n_embd:  360 # don't forget that those will be divided between the heads, so if the embedding is of size 40 and you have 4 heads, it's 10 featurs for each which is sort of riddiculous
  bias: 0 #the original nanoGPT code says that turning it off produces better results
  dropout_p: 0.1 #nanoGPT original code says 0.0 is good for pre-training, 0.1 is good for fine-tuning
  replace_head:
    cluster_idx_heads: 0 # if >0, replaces the corresponding heads with an RLMLP with h_dim specified by this value
    cluster_offset_head: 0 # if >0, replaces the corresponding heads with an RLMLP with h_dim specified by this value
input_normalization:
  normalize: {"bds": True, "obs": True} #action normalization is unavailable
  env_type: "navigation_env" #only needed if the normalize dict above has keys that are True. Currenlty, the only available choice in 'navigation_env'
train_cfg:
  data_path_train: "/home/achkan/datasets/qd_llm/per_dim_clusters_action_repr/4_4/merged_12345/archive_transformed_actions_train.pkl"
  data_path_val: "/home/achkan/datasets/qd_llm/per_dim_clusters_action_repr/4_4/merged_12345/archive_transformed_actions_val.pkl"
  max_epochs: 5000 # total number of training iterations
  val_frequ: 5 #compute validation loss only every val_frequ iterations
  batch_size: 30
  ablate_prompt: False #for bd-based only pretraining
  adamW: 
      learning_rate : 5e-3 # max learning rate 
      weight_decay : 1e-1
      beta1 : 0.9
      beta2 : 0.95
      grad_clip : 0.0 # clip gradients at this value, or disable if == 0.0
  schedule:
    decay_lr: True
    lr_decay_steps : 300000 # 
    warmup_steps: 1000 # how many steps to warm up for
    min_lr : 5e-5 # minimum learning rate, should be ~= learning_rate/10 per Chinchilla
  only_train: {"cluster_idx_heads": False, "cluster_offset_head": False} #disables backbone training and only opitmizes the chosen heads. Not that both values can be True at once.
test_cfg:
  data_path: "/home/achkan/datasets/qd_llm/per_dim_clusters_action_repr/4_4/merged_12345/archive_transformed_actions_test.pkl"
  #you can provide a list of indexes into the test archive that will be skipped (useful if you want to remove some generated examples that are too close to the train data, to avoid leakage)
  exclude_file: "/tmp/exclude_file.json" 
  batch_size: 16
  ablate_prompt: False
deploy_cfg:
  env_type: "navigation_env"
  #the file should contain a List[List] prompt_lst, such that prompt_lst[i]=[textual_conditioning, bd_0, ..., bd_n] for n-dimensional bds
  #use --export_as_prompt_list from data_tools.py
  prompts: "/tmp/logdir/prompt_file_test.json" 
  generation_strategy: "sample" #choices are 'sample', 'argmax' and 'nucleus'. For now, 'sample' seems to perform better. Also note the temperature param hardcoded in the code (it is left to 1.0)
  ablate_text: False
  ablate_bd: False 
device: "cuda"
dtype: "float32" 
#pretrained_model: ""
pretrained_model: "/home/achkan/Desktop/tmp_desktop/train_val_log_3643923/model_525"
#### note on available models
#### 3643923 => almost decent offset (0.12), good cluster id (0.038), overall one of the best stats up until now. However, not sufficient at all
#### 2110830 => good offset (0.005), bad cluster id (0.08, about 50%), 10x8 clusters
#### 1906906  => okayish offset (0.009), okayish cluster (0.042, equivalent to ~57%), 6x5
#### 27987667 => meh offset (0.019), almost good cluster (0.021, equivalent to ~63%), 3x3 clusters
#pretrained_model: "/home/achkan/Desktop/tmp_desktop/train_val_log_2798767/model_335" #with 3,3 clusters. Good cluster id loss, bad offset loss 
#note: 190606 and _1974216 have the best stats up until now, the latter was cut early. Both with 6,5 clusters
#pretrained_model: "/home/achkan/Desktop/tmp_desktop/train_val_log_1962430/model_40" #same as large one below but with larger offset head (h_sz of 240)
#pretrained_model: "/home/achkan/Desktop/tmp_desktop/train_val_log_1906906/model_495" #same as the previous large model with n_emb=360, but with less clusters (6,5) => heads were too small
###older models
#pretrained_model: "/home/achkan/Desktop/tmp_desktop/train_val_log_3381413/model_520" #large model trained with bds only, without textual conditioning => it is shit and "fine-tuning" wont help
#pretrained_model: "/home/achkan/Desktop/tmp_desktop/train_val_log_3310812/model_555" #large model, with an extra dataset with good annotations => not worth it
#medium sized model, with n_embd 320 (doesn't work as well as the largest model. However, note that we used more dropout with it too (0.2 instead of 0.1)
#pretrained_model: "/home/achkan/Desktop/tmp_desktop/train_val_log_2179516/model_595"
#Even larger model (n_emb 360) trained with normalized (per dim) focal loss from scratch, no masking, with per dim kmeans. Best stats overall
#pretrained_model: "/home/achkan/Desktop/tmp_desktop/train_val_log_2110830/model_510"
seed: 0
logging: 
  log_dir: "/home/achkan/Desktop/tmp_desktop/"
