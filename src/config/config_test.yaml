---
train_model: True #if False, a pretrained model should be provided.
test_model: False #
deploy_in_env: True 
model_cfg: 
  learned_tokenizer: "/home/achkan/misc_libs/LanguageGroundedQD_venvs/venv_1/src/LanguageGroundedQD/src/models/tokenizers/tokenizer_hz7RY06aNX"
  kmeans:  "/home/achkan/misc_libs/LanguageGroundedQD_venvs/venv_1/src/LanguageGroundedQD/src/models/kmeans/cluster_centers_per_dim_10_8.pkl"
  block_size: 1435 #In the current dataset, the max token length is 226, and the toy env's episode length is 400 (=>1200 tokens), so block_size needs only be larger than 1426.
  n_layer: 2
  n_head: 4
  n_embd:  480 # don't forget that those will be divided between the heads, so if the embedding is of size 40 and you have 4 heads, it's 10 featurs for each which is sort of riddiculous
  bias: 0 #the original nanoGPT code says that turning it off produces better results
  dropout_p: 0.1 #nanoGPT original code says 0.0 is good for pre-training, 0.1 is good for fine-tuning
input_normalization:
  normalize: {"bds": True, "obs": True} #action normalization is unavailable
  env_type: "navigation_env" #only needed if the normalize dict above has keys that are True. Currenlty, the only available choice in 'navigation_env'
train_cfg:
  data_path_train: "/home/achkan/datasets/qd_llm/per_dim_clusters_action_repr/10_8/merged_12345/archive_transformed_actions_train.pkl"
  data_path_val: "/home/achkan/datasets/qd_llm/per_dim_clusters_action_repr/10_8/merged_12345/archive_transformed_actions_val.pkl"
  max_epochs: 5000 # total number of training iterations
  val_frequ: 5 #compute validation loss only every val_frequ iterations
  batch_size: 28
  ablate_prompt: False #for bd-based only pretraining
  adamW: 
      learning_rate : 5e-3 # max learning rate 
      weight_decay : 1e-1
      beta1 : 0.9
      beta2 : 0.95
      grad_clip : 0.0 # clip gradients at this value, or disable if == 0.0
  schedule:
    decay_lr: True
    lr_decay_steps : 500000 # 
    #lr_decay_steps : 50000 # 
    warmup_steps: 5000 # how many steps to warm up for
    min_lr : 5e-5 # minimum learning rate, should be ~= learning_rate/10 per Chinchilla
test_cfg:
  data_path: "/home/achkan/datasets/qd_llm/per_dim_clusters_action_repr/10_8/merged_12345/archive_transformed_actions_test.pkl"
  #you can provide a list of indexes into the test archive that will be skipped (useful if you want to remove some generated examples that are too close to the train data, to avoid leakage)
  exclude_file: "/tmp/exclude_file.json" 
  batch_size: 16
  ablate_prompt: False
deploy_cfg:
  env_type: "navigation_env"
  #the file should contain a List[List] prompt_lst, such that prompt_lst[i]=[textual_conditioning, bd_0, ..., bd_n] for n-dimensional bds
  #use --export_as_prompt_list from data_tools.py
  prompts: "/tmp/logdir/prompt_file_test.json" 
  generation_strategy: "sample" #choices are 'sample', 'argmax' and 'nucleus'. For now, 'sample' seems to perform better. Also note the temperature param hardcoded in the code (it is left to 1.0)
  ablate_text: False
  ablate_bd: False 
device: "cuda"
dtype: "float32" 
pretrained_model: ""
#pretrained_model: "/home/achkan/Desktop/tmp_desktop/train_val_log_3381413/model_520" #large model trained with bds only, without textual conditioning => it is shit and "fine-tuning" wont help
#pretrained_model: "/home/achkan/Desktop/tmp_desktop/train_val_log_3310812/model_555" #large model, with an extra dataset with good annotations => not worth it
#medium sized model, with n_embd 320 (doesn't work as well as the largest model. However, note that we used more dropout with it too (0.2 instead of 0.1)
#pretrained_model: "/home/achkan/Desktop/tmp_desktop/train_val_log_2179516/model_595"
#Even larger model (n_emb 360) trained with normalized (per dim) focal loss from scratch, no masking, with per dim kmeans. Best stats overall
#pretrained_model: "/home/achkan/Desktop/tmp_desktop/train_val_log_2110830/model_510"
seed: 0
logging: 
  log_dir: "/home/achkan/Desktop/tmp_desktop/"
