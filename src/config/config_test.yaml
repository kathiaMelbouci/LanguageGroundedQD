---
train_model: True #if False, a pretrained model should be provided.
test_model: True  #
model_cfg:
  learned_tokenizer: "/home/achkan/misc_libs/LanguageGroundedQD_venvs/venv_1/src/LanguageGroundedQD/src/models/tokenizers/tokenizer_XT7jMcHDYG/" 
  block_size: 324
  n_layer: 4
  n_head: 4
  n_embd: 128 
  bias: 0 #the original nanoGPT code says that turning it off produces better results
  dropout_p: 0.0 #nanoGPT original code says 0.0 is good for pre-training, 0.1 is good for fine-tuning
train_cfg:
  data_path: "/home/achkan/misc_libs/ToyEnvironmentDescribedPolicyRepertoire/described_archive_full_train.pkl"
  hyperparams:
    batch_size: 4
    lr: -1
  adamW: 
      learning_rate : 6e-4 # max learning rate
      max_iters : 600000 # total number of training iterations
      weight_decay : 1e-1
      beta1 : 0.9
      beta2 : 0.95
      grad_clip : 0.0 # clip gradients at this value, or disable if == 0.0
  schedule:
    decay_lr : True # whether to decay the learning rate
    warmup_iters : 2000 # how many steps to warm up for
    lr_decay_iters : 600000 # should be ~= max_iters per Chinchilla
    min_lr : 6e-5 # minimum learning rate, should be ~= learning_rate/10 per Chinchilla
test_cfg:
  data_path: "/home/achkan/misc_libs/ToyEnvironmentDescribedPolicyRepertoire/described_archive_full_test.pkl"
  batch_size: 4
device: "cuda"
dtype: "float32"
logging: 
  pretrained_model: ""
  save_weights_to: "/Desktop/tmp_desktop/"

  







